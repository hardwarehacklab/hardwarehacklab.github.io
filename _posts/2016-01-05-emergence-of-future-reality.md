---
title        : "Exploring Virtual Reality and 3D Video"
author       : Andrew McWilliams
author_slug  : andrew-mcwilliams
author_photo : andrew.jpg
---
<p>The tech consortium <a href="http://www.nycmedialab.org/">NYC MediaLab</a> just released an interesting report on the <a href="http://www.nycmedialab.org/wp-content/uploads/2015/12/12.14.VR_.pdf">current state of Virtual Reality</a>. It looks at some of the tech, a timeline of recent events and discussion on business opportunities.</p>

<figure>
  <img alt="The Jaunt Neo" src="/images/blog/2015-06-01-vr/jaunt-neo.jpg">
  <figcaption class="caption">
    The Jaunt Neo 3D camera system
  </figcaption>
</figure>

<p>In this article I'll summarize some of the tech and experiential findings, and I'll dig in a little on some of the fascinating developments in 3D video.</p>

<!--excerpt-ends-->

<h2>Future Reality?</h2>

<p>Before we start, it's worth noting the report kicks off with the phrase 'Future Reality', borrowed from Ken Perlin, as an alternative to 'Augmented Reality' and 'Virtual Reality'.</p>

<p>Future Reality is an odd phrase. It's really trying to say that this isn't a <em>modified version of</em> reality, this will <em>actually be</em> reality - once the tech is ubiquitous.</p>

<p>This is helpful when you are thinking medium/long term, but in the short/medium time frame this report is really looking at it doesn't work. The rest of the report only talks about VR. To learn about this 'future reality' trajectory check out my <a href="http://jahya.net/blog/future-visions-for-human-interaction/">write-up of Ken Perlin's ideas here</a>.</p>

<h2>The visceral experience</h2>

<p>As for interesting things happening right now, the report discusses the difference audience members feel with VR versus regular video. It points to the development of studio-freindly 3D video and production platforms like <a href="http://www.jauntvr.com/">Jaunt</a>.</p>

<figure>
  <div class="video ratio-55">
    <iframe src="//www.youtube.com/embed/FFnhMX6oR1Q"></iframe>
  </div>
  <figcaption class="caption">
    Clouds over Sidra
  </figcaption>
</figure>

<p>The 3D video <a href="https://www.youtube.com/watch?v=FFnhMX6oR1Q">Clouds over Sidra</a> was shot with hardware like this. It allows you to spend time with a 12-year old Syrian refugee in Jordan. At the other end of the spectrum, <a href="https://www.youtube.com/watch?v=Oh2V_-HsfAs">Game of Thrones</a> is a synthetic, animated environment designed to portray a fictional world.</p>

<p>In both, the experience is more visceral for the audience. The report says:</p>

<blockquote><em>
"With virtual reality, the transaction between an audience and the story is fundamentally changed – and pared down. Established mediums ... still rely on the removed - and unmoved – perspective of their viewer. Virtual reality bridges that gap..."
</em></blockquote>

<p>The language here is one of the birth of a new medium. It reminds me of the stories of early silent film, in which cinema audiences <a href="https://www.youtube.com/watch?v=d_9N68MO9gM">were said to panic</a> at the sight of a train coming towards them (on the screen).</p>

<p>Whether or not those audiences actually panicked, they probably had the same sort of bodily reactions to the ones we have now when we experience VR. This comparison makes me wonder how much these reactions will wear off as people become accustomed to VR over time.</p>

<h2>Realtime 3D video streaming</h2>

<p>The report also mentions investment in <a href="http://techcrunch.com/2015/11/11/vr-live-streaming-startup-nextvr-raises-30-5m-series-a/">live streaming company NextVR</a>. Leading on from this, it's not hard to imagine 3D video being distributed in a similar way to Netflix or Skype now.</p>

<figure>
  <img alt="Streaming video via NextVR" src="/images/blog/2015-06-01-vr/nextvr.jpg">
  <figcaption class="caption">
    Streaming video via NextVR (credit <a href="https://www.flickr.com/photos/evrydayvr/17964868601/">evrydayvr</a> / <a href="https://creativecommons.org/publicdomain/zero/1.0/">public domain</a>)
  </figcaption>
</figure>

<p>For this to make sense the broadcaster would need to have at least one 3D camera. I don't see this becoming a norm for Skype calls. But perhaps things like <a href="http://techcrunch.com/2015/07/28/nextvr/">sporting events</a> and concerts - especially if you have the option to switch between cameras at will.</p>

<h2>The State of VR today</h2>

<p>The report has a handy 'state of play' section that goes over the birds-eye view of the industry.</p>

<h2>1. Distribution</h2>

<p>It begins with distribution, for which the challenges appear to be bandwidth, and horsepower of playback devices such as phones. As you can imagine these are all below par right now, but expected to ramp up to the required levels in the 2-5 year timeframe.</p>

<p>Another distribution problem is how users find content. Currently there is relatively little content and it is distributed inconsistently over several small distribution outlets. Typically users find content through an app store or targeted promotional outreach.</p>

<h2>2. Display</h2>

<p>This is the part that most of us have already heard about, whether you are in the industry or not. But for completeness, let's go over it.</p>

<p>Displays are divided into smartphone-based, like the <a href="http://www.samsung.com/global/galaxy/wearables/gear-vr/">Samsung GearVR</a> and <a href="https://www.google.com/get/cardboard/">Google Cardboard</a> and non-smartphone based like the <a href="https://www.playstation.com/en-au/explore/ps4/features/playstation-vr/">Sony PlayStation VR</a>, <a href="http://www.htcvive.com/">HTC Vive</a> and <a href="https://www.oculus.com/">Oculus Rift</a>.</p>

<figure>
  <img alt="Unboxing Google Cardboard" src="/images/blog/2015-06-01-vr/google-cardboard.jpg">
  <figcaption class="caption">
    Unboxing Google Cardboard (credit <a href="https://www.flickr.com/photos/brownpau/18356133005/">brownpau</a> / <a href="https://creativecommons.org/licenses/by/2.0/">cc</a>)
  </figcaption>
</figure>

<p>Of the latter three headsets, only the Oculus is seriously discussed, because it's community have driven the way in this space. The benefit with a unit like this is the sleek experience. The drawbacks are cost and platform requirements.</p>

<p>What this means is it's generally better to be running Windows on a beefed up machine. By contrast, the smartphone-based systems are cheap and run with standard phones.</p>

<h2>3. Production</h2>

<p>Here's where it gets kind of interesting, and I'll mix in a little of my own research here. How do you produce content and what are the features of each technique?</p>

<p>The report says you have three options to produce VR content: record video, animate 3D or a hybrid between the two. That's true but in reality there's a fair gray area in the hybrid space.</p>

<h2>3a. Recording 3D video</h2>

<p>When it comes to video, the standard approach is to build a rig using 6 or more cameras facing different directions. You can see this in action with GoPro cameras hooked up using 360 Hero products.</p>

<figure>
  <div class="video ratio-55">
    <iframe src="//www.youtube.com/embed/Q_zyT7scFws"></iframe>
  </div>
  <figcaption class="caption">
    GoPro and 360 Hero
  </figcaption>
</figure>

<p>This is great for giving you an immersive 3D 'bubble' to explore, but it gets frustrating for users because although you can <em>look around</em> the bubble, you can't move. The action is recorded from one spot via live synchronised video streams, so you have to be wherever the camera was.</p>

<p>You can get around this by recording with two or more camera rigs in different locations. Then you have multiple spots to view the action from. But you have to flick awkwardly between fixed camera rig positions, and smooth transitions are seemingly not possible.</p>

<h2>3b. A hybrid approach</h2>

<p><a href="http://matterport.com">Matterport</a> hardware is all about architectural mapping. The product is basically three <a href="http://jahya.net/blog/how-depth-sensor-works-in-5-minutes/">primesense sensors</a> and a hardware/software production suite.

<figure>
  <img alt="Matterport product shot" src="/images/blog/2015-06-01-vr/matterport.png">
  <figcaption class="caption">
    Matterport product shot
  </figcaption>
</figure>

<p>Here's how it works. First, it uses the mounted <a href="http://jahya.net/blog/how-depth-sensor-works-in-5-minutes/">depth sensors</a> and RGB camera to allow you to build a grainy but accurate mesh of an architectural interior. This is a bit like the <a href="http://structure.io/">Structure sensor</a> but specifically designed for architectural mapping.</p>

<p>Second, at set locations within that space it allows you to take what amount to 360 degree images. This part is effectively the same as the panorama apps on your phone. The end result is that you have a series of hi-res 3D 'bubbles' situated inside a lo-res navigable space.</p>

<figure>
  <div class="video ratio-56">
    <iframe src="https://player.vimeo.com/video/131476718?title=0&amp;byline=0&amp;portrait=0&amp;color=15a6ab&amp;hd_off=0" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
  </div>
  <figcaption class="caption">
    Matterport goes VR
  </figcaption>
</figure>


<p>This technique is pretty cool, however there are a couple of drawbacks. First, you can't do live action with it, since you don't have enough camera positions to take in the entire 360 degrees without moving the camera.</p>

<p>Second, as a default, you end up with a Google StreetView-style navigation between the 'bubbles' in the space. For fully 'immersive' applications this is real pain and can feel clunky.</p>

<p>To get past this, Matterport offer a <a href="http://matterport.com/technology/#cloud">cloud service</a> to which you upload your captured content. There they run a series of intensive computer vision algorithms, merging the hi-res 'bubble' data with the lo-res map. This produces what they say is a smooth and 'highly realistic' 3D mesh, which you can then traverse freely.</p>

<p>That sounds great but I've yet to see a solid example of it. It probably takes a while. The video example of it on the Matterport website is really small, and all the promo videos seem to use some form of StreetView-style navigation.</p>

<h2>3c. Another depth video hybrid</h2>

<p>The <a href="http://www.rgbdtoolkit.com/">DepthKit</a> hardware allows you to record live-action volumetric video which is <em>not</em> 360 degrees. It uses the same technique that Matterport uses to build it's grainy map, and embraces the graininess and lack of 360.</p>

<p>The guys at the <a href="http://www.specular.cc/">Specular studio</a> who made the DepthKit have been demonstrating that you can do interesting things even within these limitations. Check out the 'Blackout' kickstarter video:</p>

<figure>
  <div class="video ratio-56">
    <iframe src="https://player.vimeo.com/video/148961704?title=0&amp;byline=0&amp;portrait=0&amp;color=15a6ab&amp;hd_off=0" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
  </div>
  <figcaption class="caption">
    'Blackout' by Specular
  </figcaption>
</figure>

<p>To situate the watcher in an immersive 360 environment they have meticulously recreated a subway car as a virtual model based on photos of the real thing. This allows you to navigate the virtual space like a game, getting you away from Streetview navigation.</p>

<p>On top of that they apply the grainy live-action technique, filming people in a studio who are then 'placed' in the subway car. These people have occlusions - portions of their bodies you can't see. However they do have a <em>realness</em> that animated characters don't usually have.</p>

<h2>3d. The obvious one</h2>

<p>Last but not least is just building 3D virtual environments and animated characters. Here you are talking about established game development platforms like <a href="https://unity3d.com/">Unity</a> and <a href="https://www.unrealengine.com/what-is-unreal-engine-4">Unreal</a>, or web platforms like <a href="https://en.wikipedia.org/wiki/WebGL">WebGL</a> and <a href="http://threejs.org/">three.js</a>.</p>

<p>This gives you a lot of creative control, but it's a long way from live action. These environments are much more mature as people have been playing 3D computer games and rendering 3D graphics for a long time now. It's not a huge stretch to now explore that in VR.</p>

<h2>They grow up so fast</h2>

<p>I'll end with the generic get-out clause that justifies interest in emerging technology. These trajectories are moving quickly, and converging on each other. It's easy to dismiss technologies when they are at an immature stage, but I'd say keep your eye on them, and learn about these limitations.</p>

<p>They will mature, so the only question is whether you want to hang back and let everyone else to figure it out before you get involved.</p>